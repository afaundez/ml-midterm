\documentclass[letterpaper, conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{mathabx}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{pgfplots}
\pgfplotsset{compat=1.5.1}

%\usepackage{titlesec}
%\titlespacing*{\subsection}{0pt}{1.1\baselineskip}{\baselineskip}
%\setlength{\parindent}{2em}
%\setlength{\parskip}{0.7em}

\usepackage{url}

\begin{document}

\title{A Discrete Bayesian Classifier \\
  \large Machine Learning Midterm Report}

\author{
  \IEEEauthorblockN{Alvaro Faundez}
  \IEEEauthorblockA{
    \textit{Master in Data Science's Program, first year}\\
    \textit{CUNY Graduate Center}\\
    \textit{alvaro@faundez.net}
  }
}

\maketitle

\begin{abstract}

This report details the implementation of a discrete Bayesian classifier. The classifier requirements are a dataset and an economic gain matrix for input. The process calculates the class, measurement conditional, and class conditional probabilities from the dataset, all needed for the Bayes theorem. It outputs the Bayes rule, the confusion matrix, and the expected gain matrix associated with the input used. After completing the training and predicting steps, small perturbations are introduced in the measurement conditional probabilities of every misclassification, obtaining an improvement, as expected, in the expected gain (after recalculating all the classifier's outputs). The experiments included using the dataset split into equally sized training and test sets and using K-fold cross-validation. The dataset was created using synthetic probabilities distribution. A series of results are detailed in the appendix.

\end{abstract}

\section{Introduction}


The discrete Bayesian classifier is a popular predictive classifier broadly used in Machine Learning, based on the Bayes Theorem [\ref{eq:bayes-theorem}].

\begin{equation}\label{eq:bayes-theorem}
  P(A \mid B) = \frac{P(B \mid A)\mathbin{}P(A)}{P(B)}
\end{equation}

A discrete Bayesian classifier, given a set of classifications, a measurement space, and a dataset, uses the Bayes theorem to compute the conditional probabilities and the Bayes rule decision needed to perform classification. It can be optimized using an Economic Gain matrix, by maximizing the Expected Gain obtained using the Bayes decision rule.

In our case, given a set $C$ of $K$ discrete classifications [\ref{eq:classes-set}] and a discrete measurement space $D$, result of the cartesian product of $N$ discrete measurements $L_n$ [\ref{eq:measurement-space}], the classifier is a function assigning a unique classification $c \in C$ to any measurement $\vec{d} \in D$.

\begin{equation}\label{eq:classes-set}
C = \{\,c_1,\, \dots,\, c_{K}\,\}
\end{equation}


\begin{gather}\label{eq:measurement-space}
D = \bigtimes_{n=1}^{N} L_n \\
L_n = \{l_{n_1},\, \dots,\, l_{n_{M_n}}\},\, \forall n \in { 1, ..., N }, M_n = \vert L_n\vert
\end{gather}

For the classification, the following inputs are required:

\begin{itemize}
  \item the discrete classification cardinality, $K$
  \item the cardinalities of each discrete measurement, $\vert L_n \vert,\, n \in N$
  \item an Economic Gain Matrix, $e^{K \times K}$
  \item a dataset with $Z$ measurements with matching $Z$ classifications. 
\end{itemize}

The Economic Gain matrix, $e$, is a $K \times K$ matrix that defines the gain (or cost) of making the right (or wrong) classifications [\ref{eq:economic-gain-matrix}]. Each column will represent an assigned class, and each row will represent the true class. Then, every element $e(c_i,\, c_j) \in e$ represent the gain or cost of assigning the class $c_j$ when the true class was $c_i$. The economic gain usually is positive for every correct classification and non-positive for every other case. The identity matrix is an example of an economic gain matrix.

\begin{equation}\label{eq:economic-gain-matrix}
  e^{K \times K} = \begin{pmatrix}
    e(c_1,c_1) & \cdots & e(c_1,c_K) \\
      \vdots   & \ddots &    \vdots   \\
    e(c_K,c_1) & \cdots & e(c_K,c_K)
  \end{pmatrix}
\end{equation}


The economical consequences of the classifier are determined by the Expected Gain Matrix $G^{K \times K}$. Each element $g$ is the multiplication between the economic gain $e(i,\, j) \in K$ and the probability of the true class $c_i$ being assigned class $c_j$, $P(c_i,\, c_j)$ [\ref{eq:expected-gain-matrix}].

\begin{equation}\label{eq:expected-gain-matrix}
  G^{K \times K} = \begin{pmatrix}
    e(c_1,c_1)P(c_1,c_1) & \cdots & e(c_1,c_K)P(c_1,c_K) \\
                \vdots   & \ddots &    \vdots   \\
    e(c_K,c_1)P(c_K,c_1) & \cdots & e(c_K,c_K)P(c_K,c_K)
  \end{pmatrix}
\end{equation}

The Expected Gain of the classifier is the sum of all the elements of the Expected Gain Matrix [\ref{eq:expected-gain}].

\begin{equation}\label{eq:expected-gain}
  E[e] = \sum_{i \in K} \sum_{j \in K}e(c_i,\,c_k)\mathbin{}P(c_i,\,c_k)
\end{equation}

The goal is to determine the Bayes decision rule that maximizes the Expected Gain of the classifier, $E$.

A critical step in the construction of the classifier is the building of the Bayes decision rule $f_{\vec{d}}$ [\ref{eq:bayes-rule}]. The Bayes decision rule will assign 1 to the class that maximizes the Expected Gain [\ref{eq:bayes-argmax}], and 0 otherwise.

\begin{equation}\label{eq:bayes-rule}
  f_d(c_j) =
  \begin{cases}
  1 & j = k \\
  0 & j \neq k
  \end{cases}
\end{equation}

\begin{equation}\label{eq:bayes-argmax}
  \argmax_{c_k \in C} \sum_{j = 1}^{K} e(c_j, c_k)\mathbin{}P(c_j, \vec{d})
\end{equation}

The Expected Gain can be written in function of the Bayes decision rule [\label{eq:bayes-expected-gain}].

\begin{equation}\label{eq:bayes-expected-gain}
  E[e, f] = \sum_{i \in K} \sum_{j \in K}\sum_{\vec{d} \in D}f_{\vec{d}}(c_j)\mathbin{}e(c_i,\, c_j)\mathbin{}P(c_i,\,\vec{d})
\end{equation}

To obtain $f_{\vec{d}}$ and $P(c,\, \vec{d})$, it is necessary to calculate the posterior probability of assigning a class $c$ to measurement $\vec{d}$, $P(c \mid \vec{d})$, using the prior class probability $P(c)$ and the probability of the measurement given the class $P(\vec{d} \mid c)$ [\ref{eq:bayes-theorem-proportional}].

\begin{equation} \label{eq:bayes-theorem-proportional}
  P(c,\, \vec{d}) \mathbin{\propto} P(\vec{d} \mid c) \mathbin{} P(c)
\end{equation}

This report details an implementation of a discrete Bayesian classifier used to implement the classifier. A technical overview explains the design and implementation, along with the results of the experiments executed.

The definition and implementation include:

\begin{itemize}
  \item Classification and measurement dimensions, including pseudo-random probabilities and cumulative distribution functions
  \item Space definition and linear addresses
  \item Class prior and conditional probabilities
  \item Classifier optimization using an economic gain matrix
  \item Dataset definition and generation of pseudo-random synthetic data
  \item Experimentation framework with two types of cross-validation
\end{itemize}

The classifier's performance is tested by adding small perturbations to the class conditional, increasing the expected gain. The increase was tested using are training/test and k-folds cross-validation.

\section{Technical}

In order to explain the implementation, a few concepts must be discussed.

A experiment will be defined as the isolated instance that takes inputs and produce results. The experiments will defined dimensions, classifications, measurement space, classifier, and datasets.

A dimension is a 1-dimensional set of $M$ correlatives integer numbers from 1 to $K$. Any 1-dimensional $L$ set has a Probability Mass Function $P$ and a Cumulative Distribution Function $Q$. The $P$ set will be created by generating a set $R$ with $K$ pseudo-random numbers $r_1, ...r_K$ scaled to 1 $p_1, ...r_K$ [\ref{eq:pmf}]. The $Q$ set is sequence of cumulative sum of the probabilities in the $P$ [\ref{eq:cdf}].

\begin{equation}\label{eq:pmf}
  \begin{aligned}
  R &= \{r_1, \dots, r_K\} \\
  P &= \Bigl\{p_i \mid p_i = \frac{r_i}{r_{\sum_{i = 1}^{K} r_i}}, r_i \in R \Bigr\}
  \end{aligned}
\end{equation}

\begin{equation}\label{eq:cdf}
Q = \Bigl\{q_i \mid q_i = \sum_{j = 1}^{K} p_j, p_j \in P \Bigr\}
\end{equation}

The classifications set $C$ and every measurement $L_n$ are 1-dimensional set with their corresponding probabilities sets.

The cumulative distribution functions will be used to generate pseudo-random numbers in the 1-dimensional space: Given a cumulative distribution function $Q$ belonging to the dimension $M$, $f_Q$ will take a number $r \in [0, 1]$. $f_Q$ and assign it the number $k \in M$ according to the relative position of $r$ compared with the numbers in $Q$ [\ref{eq:cummulative}].

\begin{equation}\label{eq:cummulative}
  f_Q(r) =
  \begin{cases}
  1 & r < q_1 \\
  k & r q_{k - 1} \leq q_k,\, k \in M - \{1\} \\
  \end{cases}
\end{equation}

The measurement space $D$ is the cartesian product of a set of measurements \label{eq:measurement-space}. Every element in $\vec{d} \in D$ is a vector where every value corresponds to a measurement value.

\begin{equation}
  \vec{d} = (d_0, ..., d_N), \vec{d}_i \in L_n
\end{equation}

A sample dataset $XY$ is formed by two sets of equal size $Z$: the data $ X = \{x_i \in D \mid i \in [1, Z]\}$ and the target $Y = \{ y_i \in C \mid i \in [1, Z]\}$.






\subsection{Requirements}

To run the code, a Unix environment with Ruby 2.6+ is necessary. The code is available on GitHub at \url{https://github.com/afaundez/ml-midterm}.

To test the command and check the available options run:

\begin{verbatim}
  $ ./midterm --help
\end{verbatim}

\subsection{options}

For testing, the following parameters are available for configuration:

\begin{itemize}
    \item-s, --seed [INT]                 Pseudo-random seed, an integer. Default: nil
    \item-c, --classes [INT]              Class cardinality, an integer. Default 2
    \item-m, --measurements [INT]         Measurements size, an integer. Default 5
    \item    --measurement-min-cardinality [INT]
                                     Measurement Min Cardinality, an integer. Default 3
    \item    --measurement-max-cardinality [INT]
                                     Measurement Max Cardinality, an integer. Default 6
    \item    --sample-size [INT]          Sample size, an integer. Default to 10 times the  space addresses size.
    \item-i, --iterations [INT]           Iterations, an integer. Default 2
    \item-d, --delta [FLOAT]              Delta for conditionals improvement, a float. Default: 0.01
    \item    --no-overlap                 Generate classes based on measurements
    \item    --uniform                    Use uniform distribution on all dimensions
\end{itemize}

\subsection{Runtime}

\subsubsection{Build}

During the build, three significant abstractions are used: Dimension, Space, and DataSet.

The Dimension abstraction is meant to store the cardinality, probabilities distribution function, and cumulative distribution function of a single measurement or class. A dimension does not store values, but it can be used to generate random values using the distribution functions. A Dimension is defined by:

\begin{itemize}
  \item size
  \item pdf, generated based on the size
  \item cdf, generated based on the pdf
  \item distribution, whether to use a random or uniform distribution
\end{itemize}

The Space abstraction stores the single class dimension \ref{classes}, the collection of measurements dimensions \ref{dimensions}, and the class conditional probabilities. Since a space knows all the dimensions specifications, it is in charge of translating and measurement into a linear address and vice-versa. A space is defined by:

\begin{itemize}
\item class\_dimension, a Dimension instance
\item measurements\_dimensions, a collection of Dimension instances
\item likelihoods, random pdfs
\end{itemize}

The DataSet abstraction generates and stores measurement values and the associated classification, all bounded to a specific space. Since it has access to the data and space, it is in charge of the training and improving the class conditional probabilities, determining the Bayes Decision Rule, generating the Confusion Matrix, and calculating the Expected Gain. It is relevant that at the time of creating, the prior class probabilities are calculated and stored in the DataSet. A DataSet is defined by:

\begin{itemize}
\item size
\item space, a Space instance
\item samples, a collection of collections of measurements
\item class\_outcomes, counter of classes for prior class probabilities calculation
\item overlap, whether to assign classes with overlap
\end{itemize}

The Economic Gain Matrix is generated in this step. Even though it is a method, currently, the only matrix available is the identity matrix.

\subsubsection{Training}

Once a set of DataSet is created, the test DataSet is picked and left isolated, and the rest is used as train DataSet.

The training process consists of two steps. Using the training set and the economic gain matrix, the training outputs the Bayes decision rule and the expected gain. Then, using the Bayes decision rule, it generates the confusion matrix.


At the end of this process, the Space associated with the DataSet will store the measurement conditional probabilities $P(c \mid d)$ targeted in \ref{eq:bayes}.

\subsection{Optimization}

Now that the space is trained and the measurement conditional probabilities have been processed, using the delta as input, the dataset can improve the class conditional probabilities using the test DataSet. For each mismatched classification, the corresponding class conditional probability is incremented by the delta input, and the measurement probabilities are normalized to 1. This process yields an updated Space and accuracy obtained.

\section{Experimental Results}

Even though it is not a strict test, it is good to do a sanity check with a dummy case: one feature, one value measurement. It is expected to have an expected gain of 1.0 and an accuracy of 1.0.

Running

\begin{verbatim}
  bin/midterm --classes 1 --measurements 1
\end{verbatim}

yields, as expected,

\begin{verbatim}
Economic Gain Matrix
+---+
| 1 |
+---+

Train 0.
  Expected Gain: 1.0
  Confusion Matrix Trace: 1.0
  Accuracy: 1.0
Confusion Matrix
+-----+
| 1.0 |
+-----+

Test
  Expected Gain: 1.0
  Confusion Matrix Trace: 1.0
  Accuracy: 1.0
\end{verbatim}


The next step is to check the results using. Let us use two classes, five values measurements, and ten training sets, and start setting the seed number in order to have replicable experiments.

\subsection{First results}

\begin{verbatim}
  bin/midterm --classes 2 --measurements 5 \
    --iterations 10 --seed 1234
\end{verbatim}

The expected gain and accuracy do not seem to be correct

\begin{figure}[hbt]
  \label{fig:10-training-2-classes}
  \caption{}
\end{figure}

Incrementing the training iterations to 100 does not make real improvements; it keeps going up and down.

\begin{figure}[hbt]
  \label{fig:100-training-2-classes}
  \caption{}
\end{figure}

To identify the reason of these results, two options were added to the application, one to control the overlap in the class assignment to a measurement and how the probability distribution is generated.

\subsection{Probability distributions}

Since the method used to generate the probability distribution function for a dimension does not generate even probabilities for each value in the dimension, a parameter $--uniform$ was added to revert this and generate uniform distribution for the classes and measurement. The hypothesis was that by generating numbers evenly, the results would easier to understand.

Now, repeating the previous commands with the new parameter:

\begin{verbatim}
  bin/midterm --classes 2 --measurements 5 \
    --iterations 10 --seed 1234 \
    --uniform
\end{verbatim}

\begin{figure}[hbt]
  \label{fig:10-training-2-classes-uniform}
  \caption{}
\end{figure}

The expected gain does not seem to be improving, but the accuracy is set around $0.5$. It could be a sign that accuracy is bouncing between that particular number because now each class has the same probability of being assign to a measurement. Using 100 training iterations, the result is similar.

\begin{figure}[hbt]
  \label{fig:100-training-2-classes-uniform}
  \caption{}
\end{figure}

Checking with 3 classes, the accuracy bounce around $0.33$ as expected:

\begin{figure}[hbt]
  \label{fig:10-training-3-classes-uniform}
  \caption{}
\end{figure}

\subsection{Class overlapping}

To understand if the accuracies shown in the previous steps are determined by the number of classes and the overlapping caused by an assignation independent from the measurements, it was added to the program the option to avoid the overlap and assign a class based on the norm of the measurement vector. This is an arbitrary assignment chosen just by the simplicity and certainty of the assignment's uniqueness.

using the new parameter

\begin{verbatim}
  bin/midterm --classes 2 --measurements 5 \
    --iterations 10 --seed 1234 \
    --no-overlap
\end{verbatim}

\begin{figure}[hbt]
  \label{fig:10-training-2-classes-no-overlapping}
  \caption{}
\end{figure}

Now the expected gain is considerably higher, and trying with 100 iterations stays close to 1.0.

\begin{figure}[hbt]
  \label{fig:100-training-2-classes-no-overlapping}
  \caption{}
\end{figure}

\section{Conclusions}

This report shows a Bayes classifier written from scratch in Ruby without any scientific library, following the theory and instructions provided during the Machine Learning class.

The program includes definitions and abstractions for Dimension, Space, and DataSet that can be configured with optional parameters through the command line to create classes, measurement, and training sets.

The initial results were not the one expected, showing low and bouncing expected gains and accuracy. As a way to explain these behaviors, two paths were developed

\begin{itemize}
  \item First, create the dimensions for classes and measurements using uniform probability distribution functions, expecting more recognizable numbers. The method worked, and it allowed us to determine that the accuracy was inversely proportional to the classes' cardinality.
  \item Then, with the conclusion from the previous step, it was introduced an option to assign a class to a specific measurement without overlapping. The norm of the measurement vector was the method used, and it produced a considerable impact on the expected gain and accuracy, reaching values over $0.99$ in both parameters.
\end{itemize}

Although some aspects of the value could have been explained with the overlapping of classes, it remains inconclusive why the expected gain and accuracy are not reaching 1.0 or why it does not stop bouncing.

figures/test-c10-m4-l4-r10
\input{figures/test-c10-m4-l4-r10}

figures/test-c10-m4-l4-r100
\input{figures/test-c10-m4-l4-r100}

figures/test-c10-m4-l4-r1000
\input{figures/test-c10-m4-l4-r1000}


\begin{thebibliography}{2}

\bibitem{discrete-bayes}
Robert M. Haralick,
\textit{Discrete Bayes Pattern Recognition}
\\\texttt{\url{http://haralick.org/ML/discrete_bayes.pdf}}

\bibitem{midterm-project}
Robert M. Haralick,
\textit{Midterm Project}
\\\texttt{\url{http://haralick.org/ML/midterm_project.pdf}}

\end{thebibliography}

\end{document}
