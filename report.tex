\documentclass[letterpaper, conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{mathabx}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{pgfplots}
\pgfplotsset{compat=1.5.1}

\usetikzlibrary{matrix}

\setlength{\parskip}{0.7em}

\usepackage{url}

\usepackage{listings}

\usepackage{amsfonts}

\begin{document}

\title{A Discrete Bayesian Classifier \\
  \large Machine Learning Midterm Report}

\author{
  \IEEEauthorblockN{Alvaro Faundez}
  \IEEEauthorblockA{
    \textit{Master in Data Science's Program, first year student}\\
    \textit{Graduate Center, CUNY}\\
    \textit{alvaro@faundez.net}
  }
}

\maketitle

\begin{abstract}

This report details the implementation of a discrete Bayesian classifier. The classifier requirements are a class prior probabilities, the class conditional probabilities, and an Economic Gain Matrix for input. The process calculates the measurement's conditional probabilities needed for the Bayes theorem, and it outputs the Bayes decision rule, the Confusion Matrix, and the Expected Gain Matrix associated with the input used. After completing the classifications for every measurement in a testing sample set, small perturbations are introduced in the measurement conditional probabilities of every misclassification, obtaining an improvement, as expected, in the classifier's Expected Gain (after recalculating all the classifier's outputs). The experiments include the generation of synthetic random samples, and it split them equally sized testing and validation sets, or it uses V-fold Cross-Validation.

\end{abstract}

\section{Introduction}


The discrete Bayesian classifier is a popular predictive classifier broadly used in Machine Learning. It is based on the Bayes Theorem [Equation \ref{eq:bayes-theorem}].

\begin{equation}\label{eq:bayes-theorem}
  P(A \mid B) = \frac{P(B \mid A)\mathbin{}P(A)}{P(B)}
\end{equation}

A discrete Bayesian classifier, given a set of classifications and a measurement space, uses the Bayes theorem to compute the conditional probabilities and the Bayes rule decision needed to perform classifications. It can be optimized using an Economic Gain Matrix by maximizing the Expected Gain obtained using the Bayes decision rule.

In our case, given a set $C$ of $K$ discrete classifications [Equation \ref{eq:classes-set}] and a discrete measurement space $D$, result of the cartesian product of $N$ discrete measurements $L_n$ [Equation \ref{eq:measurement-space}], the classifier is a function assigning a unique classification $c \in C$ to any measurement $\vec{d} \in D$.

\begin{equation}\label{eq:classes-set}
C = \{\,c_1,\, \dots,\, c_{K}\,\}
\end{equation}


\begin{equation}\label{eq:measurement-space}
  \begin{aligned}
  L_n &= \{l_{n_1},\, \dots,\, l_{n_{M_n}}\},\, \forall n \in \{ 1, ..., N \} \\
  M_n &= \vert L_n\vert \\
  D &= \bigtimes_{n=1}^{N} L_n \\
  \end{aligned}
\end{equation}

For the classification, the following inputs are required:

\begin{itemize}
  \item the discrete classification cardinality, $K$
  \item the cardinalities of each discrete measurement, $M_n,\, n \in N$
  \item an Economic Gain Matrix, $e^{K \times K}$
  \item a dataset with $Z$ measurements with matching $Z$ classifications. 
\end{itemize}

The Economic Gain matrix, $\mathcal{E}$, is a $K \times K$ matrix that defines the gain (or cost) of making the right (or wrong) classifications [Equation \ref{eq:economic-gain-matrix}]. Each row of the matrix represents a true classification, and each column of the matrix will represent an assigned class. Then, every element $e(c_i,\, c_j) \in e$ represent the gain or cost of assigning the class $c_j$ when the true class is $c_i$. Usually, The Economic Gain has positive for every matching classification and non-positive for every other case. The identity matrix is an example of an economic gain matrix.

\begin{equation}\label{eq:economic-gain-matrix}
  \mathcal{E}^{K \times K} = \begin{pmatrix}
    e(c_1,c_1) & \cdots & e(c_1,c_K) \\
      \vdots   & \ddots &   \vdots   \\
    e(c_K,c_1) & \cdots & e(c_K,c_K)
  \end{pmatrix}
\end{equation}

The Confusion Matrix, $\mathcal{C}$ is as $K \times K$ that tells how correct is the machine learning in the classifier. Each element $P(c_i, c_j)$ in the Confusion Matrix represents the probability that the classifier assigns the classification $c_j$ when in reality the classification is $c_i$ [Equation \ref{eq:confusion-matrix}].

\begin{equation}\label{eq:confusion-matrix}
  \mathcal{C}^{K \times K} = \begin{pmatrix}
    P(c_1,c_1) & \cdots & P(c_1,c_K) \\
      \vdots   & \ddots &   \vdots   \\
    P(c_K,c_1) & \cdots & P(c_K,c_K)
  \end{pmatrix}
\end{equation}

The economical consequences of the classifier are determined by the Expected Gain Matrix $\mathcal{G}^{K \times K}$. Each element $g$ is the multiplication between the economic gain $e(i,\, j) \in K$ and the probability of the true class $c_i$ being assigned class $c_j$, $P(c_i,\, c_j)$ [Equation \ref{eq:expected-gain-matrix}].

\begin{equation}\label{eq:expected-gain-matrix}
  \mathcal{G}^{K \times K} = \begin{pmatrix}
    e(c_1,c_1)P(c_1,c_1) & \cdots & e(c_1,c_K)P(c_1,c_K) \\
              \vdots    & \ddots &            \vdots     \\
    e(c_K,c_1)P(c_K,c_1) & \cdots & e(c_K,c_K)P(c_K,c_K)
  \end{pmatrix}
\end{equation}

The classifier's Expected Gain is the sum of all the Expected Gain Matrix's elements [Equation \ref{eq:expected-gain}].

\begin{equation}\label{eq:expected-gain}
  E[e] = \sum_{i \in K} \sum_{j \in K}e(c_i,\,c_k)\mathbin{}P(c_i,\,c_k)
\end{equation}

A critical step in the construction of the classifier is the building of the Bayes decision rule $f_{\vec{d}}:C \longrightarrow \{0, 1\}, \forall \vec{d} \in D$. The goal is to determine the Bayes decision rule that maximizes the Expected Gain classifier, $E[e]$ [Equation \ref{eq:bayes-argmax}]. The Bayes decision rule will assign 1 to the class that maximizes the Expected Gain, and 0 otherwise [Equation \ref{eq:bayes-rule}].

\begin{equation}\label{eq:bayes-argmax}
  \argmax_{c_k \in C} \sum_{j = 1}^{K} e(c_j, c_k)\mathbin{}P(c_j, \vec{d})
\end{equation}

\begin{equation}\label{eq:bayes-rule}
  f_d(c_j) =
  \begin{cases}
  1 & j = k \\
  0 & j \neq k
  \end{cases}
\end{equation}

In addition to the Economic Gain Matrix, a Bayes decision rule allows a refinement of the Economic Gain definition [\label{eq:bayes-expected-gain}].

\begin{equation}\label{eq:bayes-expected-gain}
  E[e, f] = \sum_{i \in K} \sum_{j \in K}\sum_{\vec{d} \in D}f_{\vec{d}}(c_j)\mathbin{}e(c_i,\, c_j)\mathbin{}P(c_i,\,\vec{d})
\end{equation}

To obtain $f_{\vec{d}}$ and $P(c,\, \vec{d})$, it is necessary to calculate the posterior probability of assigning a class $c$ to measurement $\vec{d}$, $P(c \mid \vec{d})$, using the prior class probability $P(c)$ and the probability of the measurement given the class $P(\vec{d} \mid c)$ [Equation \ref{eq:bayes-theorem-proportional}].

\begin{equation} \label{eq:bayes-theorem-proportional}
  P(c,\, \vec{d}) \mathbin{\propto} P(\vec{d} \mid c) \mathbin{} P(c)
\end{equation}

This report details an implementation of a discrete Bayesian classifier used to implement the classifier. A technical overview explains the design and implementation, along with the results of the experiments executed.

The definition and implementation include:

\begin{itemize}
  \item Classification and measurement dimensions, including pseudo-random probabilities and cumulative distribution functions
  \item Space definition and linear addresses
  \item Class prior and conditional probabilities
  \item Classifier validation
  \item Dataset definition and generation of pseudo-random synthetic data
\end{itemize}

The classifier's performance will be measured using the Expected Gain, and it will be tested by adding small perturbations to the class conditional, increasing the Expected Gain monotonically. The process uses Test-Validation sets or V-folds Cross-Validation as validation steps.

\section{Technical}

\subsection{Definitions}

In order to explain the implementation, a few concepts need discussion.

\subsubsection{Dimension}
A dimension is a 1-dimensional set of $M$ correlatives integer numbers from 1 to $K$. Any 1-dimensional $L$ set has a Probability Mass Function $\mathbb{P}$ and a Cumulative Distribution Function $\mathbb{Q}$ [Figure \ref{fig:pmf-cdf}]. The $\mathbb{P}$ set will be created by generating a set $\mathbb{R}$ with $K$ pseudo-random numbers $r_1, ...r_K$ scaled to 1 $p_1, ...r_K$ [Equation \ref{eq:pmf}]. The $\mathbb{Q}$ set is sequence of cumulative sum of the probabilities in $\mathbb{P}$ [Equation \ref{eq:cdf}].

\pgfplotstableread[row sep=\\,col sep=&]{
value & Probability \\
1 & 0.07155134805450836 \\
2 & 0.12997048848822798 \\
3 & 0.1386944981818227 \\
4 & 0.04197817941692496 \\
5 & 0.16869281585900325 \\
6 & 0.16136757309412517 \\
7 & 0.03288486428939286 \\
8 & 0.1571494317575896 \\
9 & 0.07917145577927193 \\
10 & 0.0185393450791332 \\
}\pmf

\pgfplotstableread[row sep=\\,col sep=&]{
value & Cumulative Probability \\
1 & 0.07155134805450836 \\
2 & 0.20152183654273634 \\
3 & 0.34021633472455903 \\
4 & 0.382194514141484 \\
5 & 0.5508873300004873 \\
6 & 0.7122549030946125 \\
7 & 0.7451397673840053 \\
8 & 0.9022891991415949 \\
9 & 0.9814606549208669 \\
10 & 1.0 \\
}\cdf

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      x label style={at={(axis description cs:0.5,-0.1)}, anchor=north},
      xlabel={Dimension values},
      legend pos=north west
      ]
      \addplot table[x=value,y=Probability]{\pmf};
      \addplot table[x=value,y=Cumulative Probability]{\cdf};
      \addlegendentry{PMF}
      \addlegendentry{CDF}
    \end{axis}
  \end{tikzpicture}
  \caption{PMF and CDF values for a dimension with 10 possible values}
  \label{fig:pmf-cdf}
\end{figure}

\begin{equation}\label{eq:pmf}
  \begin{aligned}
  \mathbb{R} &= \{r_1, \dots, r_K\} \\
  \mathbb{P} &= \Bigl\{p_i \mid p_i = \frac{r_i}{r_{\sum_{i = 1}^{K} r_i}}, r_i \in \mathbb{R} \Bigr\}
  \end{aligned}
\end{equation}

\begin{equation}\label{eq:cdf}
\mathbb{Q} = \Bigl\{q_i \mid q_i = \sum_{j = 1}^{K} p_j, p_j \in \mathbb{P} \Bigr\}
\end{equation}

The classifications set $C$, and every measurement $L_n$ is a 1-dimensional set with their corresponding probabilities sets.

The cumulative distribution functions will be used to generate pseudo-random numbers in the 1-dimensional space: given a cumulative distribution function $\mathbb{Q}_n$ belonging to the dimension $M_n$, $f_{\mathbb{Q}_n}$ will take a pseudo-random number $r \in [0, 1]$. $f_{\mathbb{Q}_n}$ and assign it the number $m \in M_n$ according to the relative position of $r$ compared with the numbers in $\mathbb{Q}_n$ [Equation \ref{eq:cumulative}].

\begin{equation}\label{eq:cumulative}
  f_{\mathbb{Q}_n}(r) =
  \begin{cases}
  1 & r < q_1 \\
  m & q_{m - 1} < r \leq q_k,\, m \in M - \{1\} \\
  \end{cases}
\end{equation}

\subsubsection{Measurement Space}
The measurement space $D$ is the cartesian product of a set of $N$ measurements dimensions [Equation \ref{eq:measurement-space}]. Every element in $\vec{d} \in D$ is a vector where every value corresponds to a measurement value [Equation \ref{eq:measurements-vector}].

\begin{equation}\label{eq:measurements-vector}
  \vec{d} = (d_0, ..., d_N), \vec{d}_i \in L_n
\end{equation}

\subsubsection{Linear Space}
Each element in the measurement space can be mapped to a linear space using a bijective function $f_D$ that takes a vector and transform it to an integer value $l \in \mathcal{L}$ [Equation \ref{eq:linear-address}].

\begin{equation}\label{eq:linear-address}
  \begin{aligned}
  \mathcal{S} &= \{1,\, 2,\, 3,\, \dots,\, \prod_{i \in N} \mid L_i \mid\} \\
  f_D&: \bigtimes_{n=1}^{N} L_n \longleftrightarrow \mathcal{S} \\
  \end{aligned}
\end{equation}

Every dimension in the measurement space is independent between each other, allowing to compute the probability of every element in the measurement space $P(\vec{d})$ as the multiplication of the probabilities of each measurement value in $\vec{d}$ [Equation \ref{eq:prod:measurements}].

\begin{equation}\label{eq:prod:measurements}
  P(\vec{d}) = \prod_{i = 1}^{N} P(d_i),\, d_i \in L_i
\end{equation}

\subsubsection{Classifier}
The classifier is the function that assigns a classification to a measurement. The classifier lives in the context of a set of classifications, measurement space, and Economic Gain Matrix.

\subsubsection{Dataset}
A sample dataset is formed by two sets of equal size $Z$: the data $ X = \{x_i \in D \mid i \in [1, Z]\}$ and the target $Y = \{ y_i \in C \mid i \in [1, Z]\}$.

\subsubsection{Experiment}
An experiment is an isolated routine that takes inputs and produces results. The experiments will define dimensions, classifications, measurement space, classifier, and datasets.

An experiment uses immutable measurement space and classifications until the end of the whole process. That condition will mean that, during an experiment, the cumulative probabilities distribution of every dimension remains unchanged. The experiments will yield a classifier that will over datasets created during the experiment.

\subsubsection{Iteration}
An iteration within an experiment will enclose the use and modification of the classifier available. Each iteration in a sequence of iterations will include the classifier's changes by the previous iteration and a new dataset.

\subsubsection{Validation}
An iteration has two kinds of testing and validation processes:
\begin{itemize}
  \item Test-Validation sets: The sample dataset will be shuffled and divided into equal-sized subsets. An iteration uses the test subset to predict and adapt; the iteration finishes using the validation set [Equation \ref{eq:test-validation}].
  \item V-fold sets: The sample dataset will be shuffled and divided into $V$ equal-sized folds. A $V$ round-robin iterations will select a different fold as a validation set, and it will concatenate the resting folds as a testing set [Equation \ref{eq:v-fold}].
\end{itemize}

\begin{figure}
  \begin{tikzpicture}
    \matrix (M) [matrix of nodes,
        nodes={minimum height = 5mm, minimum width = 2.3cm, outer sep=0, anchor=center, draw},
        column 1/.style={nodes={draw=none}, minimum width = 2.3cm},
        row sep=1mm, column sep=-\pgflinewidth, nodes in empty cells,
        e/.style={fill=blue!10}
      ]
      {
        Iteration & |[e]| & \\
      };
    \draw (M-1-2.north west) ++(0,2mm) coordinate (LT) edge[|<->|, >= latex] node[above]{Sample Dataset} (LT-|M-1-3.north east);
  \end{tikzpicture}
  \caption{Test-Validation sets, only one iteration.}
  \label{eq:test-validation}
\end{figure}

\begin{figure}\label{eq:v-fold}
  \begin{tikzpicture}
    \matrix (M) [matrix of nodes,
        nodes={minimum height = 5mm, minimum width = 1cm, outer sep=0, anchor=center, draw},
        column 1/.style={nodes={draw=none}, minimum width = 1cm},
        row sep=1mm, column sep=-\pgflinewidth, nodes in empty cells,
        e/.style={fill=blue!10}
      ]
      {
        Iteration 1 & |[e]| & & & & \\
        Iteration 2 & & |[e]| & & & \\
        Iteration 3 & & & |[e]| & & \\
        Iteration 4 & & & & |[e]| & \\
        Iteration 5 & & & & & |[e]| \\
      };
    \draw (M-1-2.north west) ++(0,2mm) coordinate (LT) edge[|<->|, >= latex] node[above]{Sample Dataset} (LT-|M-1-6.north east);
  \end{tikzpicture}
  \caption{5-fold cross-validation. Each iteration uses the blue fold as validation set and the rest folds as test set.}
\end{figure}
  

\subsection{Discrete Bayesian Classifier}

A classifier will assign a classification $c \in C$ to any measurement $\vec{d} \in D$ from the measurement space. Inside the classifier, all measurement will be translated to the corresponding linear address. The following parameters must be set during the initialization of the classifier:

\begin{itemize}
  \item The list of $S$ linear addresses $l(\vec{d})$ for each $\vec{d} \in D$ [Equation \ref{eq:linear-address}], stored in a $S$ sized vector
  \item Measurement probability $P(\vec{d})$ for each  $\vec{d} \in D$ [Equation \ref{eq:prod:measurements}], stored in a $S$ sized vector
  \item Class probability $P(c)$ for each  $c \in C$ [Equation \ref{eq:pmf}], stored in a $K$ sized vector
  \item Class conditional probabilities $P(\vec{d} \mid c)$. Each class $c \in C$ is assigned a probability mass distribution with $S$ probabilities [Equation \ref{eq:pmf}], stored in a $K \times S$ matrix
\end{itemize}

Once the classifier is ready, given an Economic Gain Matrix [Equation \ref{eq:economic-gain-matrix}], the classifier computes the values needed to classify a measurement:

\begin{itemize}
  \item $P(c \mid \vec{d})$, the probabilities of class given a measurement, stored in an $S \times K$ matrix [Equation \ref{eq:bayes-theorem}]
  \item $P(c,\, \vec{d})$ the probabilities of a class and a measurement, stored in an $S \times K$ matrix [Equation \ref{eq:bayes-theorem-proportional}]
  \item the Bayes decision rule that optimize the expected gain for the Economic Gain Matrix, stored in an $S \times K$ matrix [Equation \ref{eq:bayes-rule}]
  \item The Confusion Matrix, stored in an $K \times K$ matrix
  \item The Expected Gain Matrix, stored in an $K \times K$ matrix [Equation \ref{eq:expected-gain-matrix}]
  \item The Expected Gain, the trace of the Expected Gain Matrix
\end{itemize}

The process ends with a discrete Bayesian classifier optimized to maximize Expected Gain, given an Economic Gain Matrix.

\section{Experiment}\label{experiment}

To validate the discrete Bayesian classifier, a series of iteration will modify each measurement's probabilities given a class $P(\vec{d} \mid c$ by adding small perturbations. Each perturbation must increase the Expected Gain monotonically to 1.

Each iteration in the experiment will compute a classifier optimized for a specific Economic Gain Matrix. Using that classifier dataset of $Z$ samples measurements and corresponding classifications, it generates random measurements $\vec{d}$ assigned to random classifications $c$ using probability mass distribution $P(c \mid \vec{d})$. The sample size $Z$ will be 10 times the classifications number times the measurement space [Equation \ref{eq:sample-size}].

\begin{equation}\label{eq:sample-size}
  Z = 10 \times \mid C \mid \times \prod_{n \in N} |L_n|
\end{equation}

The classifier assigns a classification to each measurement in the test sample set. Based on those results, the conditional probabilities given a class will be modified, aiming that in the next iteration, the data generated should conform more alike to the classifications. The adaptation follows these steps:

\begin{enumerate}
  \item For each assigned classification $c'$ made, if the classification is wrong a $\Delta$ perturbation will be added to $P(\vec{d} \mid c')$\footnote{This is different to the steps defined in the midterms slides \cite{midterm-project}. Section \ref{problems} provides further information.}.
  \item Normalize to 1 each column on the $P(\vec{d} \mid c)$ matrix
  \item Compute the classifier with the new probabilities
  \item Return the Expected Gain
  \item Repeat from step 1
\end{enumerate}

Each iteration must do the same steps using the updated classifier and yield a higher Expected Gain.

\section{Experiment Results}

A base experiment will have the following default setup:

\begin{itemize}
  \item $K = 2$ classifications
  \item $N = 2$ measurements
  \item $M_n = 2, n \in N$ values for each measurement
  \item $e^{K \times K} = I_K$ as Economic Gain Matrix
  \item $\Delta = 0.01$ as a probability perturbation
  \item $R = 10$ iterations per experiment
  \item $V = 2$ folds (test/validation cross-validation)
\end{itemize}

\subsection{Default Setup}

The default case generates $Z = 80$ samples. In $R = 10$ iterations the Expected Gain goes from $0.6217987808286073$ to $0.7389581277559807$.

\subsection{Testing the Parameters}

Increasing the classifications dimension's size while keeping the rest of the default values generates a decrement in the Expected Gain [Figure \ref{fig:default-K-N2-M2-R10-D001}].

\input{figures/default-K-N2-M2-R10-D001}

Increasing the amount of measurements dimensions generates improvements on the Expected Gain but also a notorious increase of the samples generated [Figure \ref{fig:results-K2-N-M2-R10-D001}]

\input{figures/default-K2-N-M2-R10-D001}

Increasing the number of values per measurement generates improvements on the Expected Gain but also a notorious increase in the samples generated [Figure \ref{fig:results-K2-N2-M-R10-D001}]

\input{figures/default-K2-N2-M-R10-D001}

Using $R = 10$ iterations the Expected Gain goes from $0.6217987808286073$ to $0.7389581277559807$. For $R > 400$ the Expected Gain gets stuck in $0.8088281174549596$ [Figure \ref{fig:results-K2-N2-M2-R-D001}]

\input{figures/default-K2-N2-M2-R-D001}

The Expected Gain's increases faster using the default perturbation value $10^{-2}$ than using smaller ones [Figure \ref{fig:results-K2-N2-M2-R10-D}].

\input{figures/default-K2-N2-M2-R10-D}

All examples discussed above use the same probabilities mass distributions when using the same values of $K$, $M$ or $L$. For example, in every case where $K = 2$ the distribution is $P(c) = \begin{pmatrix}0.6137987267145945\\0.3862012732854055\end{pmatrix}$.

\subsection{More classifications, measurements and iterations}

The measurement space size grows exponentially with the number of values in each measurement [Figure \ref{fig:results-K2-N-M2-R10-D001}]. Using $k =10$ classifications, $N = 5$ measurements each one with $M = 5$ values, the sample size has a size of $Z = 312,500$ samples. Running $R = 100$ iterations takes about $1300$ seconds, about $4000$ times the default case time for the same number of iterations. Again, the performance is better with perturbation sized $\Delta = 0.01$ [Figure \ref{fig:custom-K10-N5-M5-R100-D}].

\input{figures/custom-K10-N5-M5-R100-D}

Over a hundred iterations, the best Expected Gain achieved is $0.9995150612331569$, using a $\Delta = 0.01$. Over a thousand iterations, the Expected Gain raises to $0.9999999999806929$ [Figure \ref{fig:custom-K10-N5-M5-R1000-D001}].

\input{figures/custom-K10-N5-M5-R1000-D001}


\subsubsection{V-fold cross-validation}

All the previous experiments divided the sample set into a test set and a validation set. V-fold Cross-Validation allows using more data for the testing, producing better results in fewer iterations [Figure \ref{fig:custom-K6-N4-M3-R1000-D001-V}].

\input{figures/custom-K6-N4-M3-R1000-D001-V}

\section{Problems}\label{problems}

Section \ref{experiment}, the experiment definition, explains that the $\Delta$ perturbation modifies to the probability of the measurement $\vec{d}$ given the assigned class $c'$ that differs from the actual class $c$ from the dataset used. The perturbation increases the probability generation of the combination $(c', \vec{d})$ in the dataset of the next iteration, at the expense of any other class $c \neq c'$, making that dataset more alike to the Bayes decision rule.

That differs from the instructions provided in \cite{midterm-project} slide 24, where it is detailed that the perturbation changes the correct class $c$ conditional probability (and not the assigned class $c'$). Using the original instructions, the Expected Gain does not show a monotonic increase as it is supposed to do. Instead, it gets stuck bouncing between different values [Figure \ref{fig:slides-K5-N4-M3-R100-D001-V3}]. An explanation for this could be that it happens because by doing the original step, in every iteration, we force the generation of more samples assigned incorrectly in the previous step, forcing the Bayes rule decision to change its decision in every iteration.

\input{figures/slides-K5-N4-M3-R100-D001-V3}

\section{Conclusions}

This report showed the definition, implementation, and experimentation of a discrete Bayes classifier.

A discrete Bayes classifier involves measurement space formed by multiple discrete measurements. It assigns each measurement a classification based on what the Bayes decision rule determines as the classification that maximizes the classifier's Expectation Gain.

The critical step of the classifier is the construction of the Bayes decision rule. In this report, the probabilities $P(c)$, $P(d \mid c)$, and an Economic Gain Matrix act as the process's input. The classifier reaches its optimum by choosing a Bayes decision rule that maximizes the Expected Gain, based on the Economic Gain Matrix used as input.

The classifier's validation involves generating synthetic datasets based on the conditional probabilities computed by the classifier. During the experimentation phase, and using the Identity Matrix as the Economic Gain Matrix, the classifier's Expected Gain tends to 1.0 when, in repeated iterations, the probabilities are modified, adding small perturbation to the class conditional probabilities. The modification will move the synthetic datasets closer to the Bayes rule decision each time the classifier generates new data.

Multiple experiments run using test-validation sets and V-fold cross-validation, achieving better performance (fewer iterations needed) using the V-fold approach.

The implementation described in this report does not follow precisely the instructions from the midterm assignment because the interpretation of that instruction did not monotonically increase the Expected Gain. Instead, an alternative is proposed, achieving the monotonic increase desired. The changes involve adding the perturbations to the class conditional probabilities of the assigned class instead of the actual class, as suggested in the assignment.

The code developed and used is available at \url{https://github.com/afaundez/ml-midterm}. The appendix describes the concepts in the code.


\appendix

To run the code, a Unix environment with Ruby 2.6+ is necessary. The code is available on GitHub at \url{https://github.com/afaundez/ml-midterm}.

To test the command and check the available options run:

\begin{verbatim}
  $ ./midterm --help
\end{verbatim}

\subsection{Options}

For testing, the following parameters are available for configuration:

\begin{lstlisting}[language=sh]
  Usage: midterm [options]
    -s [INT] Pseudo-random seed
    -K [INT] Class cardinality
    -N [INT] Measurements cardinality
    -M [INT] Measurement cardinality 
    -Z [INT] Sample size
    -R [INT] Iterations
    -D [FLOAT] Perturbation size
    -V [FLOAT] Number of folds
\end{lstlisting}

Example: the command

\begin{lstlisting}[language=sh]
  bin/midterm -s1 -K2 -N2 -M2 -R10 -D0.01
\end{lstlisting}

Runs ten iterations using two classifications, two measurements (each one with two values), and add perturbations of 0.01.

\subsection{Runtime}

\subsubsection{Build}

The build uses three significant abstractions: Dimension, Space, and DataSet.

The Dimension abstraction stores the cardinality, probabilities distribution function, and cumulative distribution function of a single measurement or class. A dimension does not store values, but it can generate random values using the distribution functions. A Dimension is defined by:

\begin{itemize}
  \item size
  \item pmf, the probability mass function
  \item cdf, the cumulative distribution function
\end{itemize}

The Space abstraction stores a collection of measurements dimensions [Equation \ref{eq:measurement-space}]. It is in charge of transforming a measurement vector into a linear address and vice-versa. Space is defined by:

\begin{itemize}
  \item dimensions, a collection of dimensions instances, one for each measurement
\end{itemize}

The DataSet abstraction generates and stores measurement values and the associated classification, all bounded to a specific measurement space and classification dimension. A DataSet is defined by:

\begin{itemize}
  \item data, a collection measurement space values
  \item target, a collection of classification labels
\end{itemize}

The classifier is in charge of the training and improving the class conditional probabilities, determining the Bayes Decision Rule, generating the Confusion Matrix, and calculating the Expected Gain.

\begin{itemize}
  \item space
  \item labels (classification dimensions)
  \item expected gain matrix (default to the identity matrix)
\end{itemize}

Using the Ruby language characteristics, the solution provides dynamic specific language specifically created for this problem. As with almost every code in Ruby, it came with a performance trade-off, but it attempts to make the code more human-readable.

\begin{thebibliography}{2}

\bibitem{discrete-bayes}
Robert M. Haralick,
\textit{Discrete Bayes Pattern Recognition}
\\\texttt{\url{http://haralick.org/ML/discrete_bayes.pdf}}

\bibitem{midterm-project}
Robert M. Haralick,
\textit{Midterm Project}
\\\texttt{\url{http://haralick.org/ML/midterm_project.pdf}}

\end{thebibliography}

\end{document}
