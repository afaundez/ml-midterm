\documentclass[letterpaper, conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{mathabx}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{pgfplots}
\pgfplotsset{compat=1.5.1}

\usetikzlibrary{matrix}

%\usepackage{titlesec}
%\titlespacing*{\subsection}{0pt}{1.1\baselineskip}{\baselineskip}
%\setlength{\parindent}{2em}
\setlength{\parskip}{0.7em}

\usepackage{url}

\usepackage{listings}

\begin{document}

\title{A Discrete Bayesian Classifier \\
  \large Machine Learning Midterm Report}

\author{
  \IEEEauthorblockN{Alvaro Faundez}
  \IEEEauthorblockA{
    \textit{Master in Data Science's Program, first year}\\
    \textit{CUNY Graduate Center}\\
    \textit{alvaro@faundez.net}
  }
}

\maketitle

\begin{abstract}

This report details the implementation of a discrete Bayesian classifier. The classifier requirements are a dataset and an economic gain matrix for input. The process calculates the class, measurement conditional, and class conditional probabilities from the dataset, all needed for the Bayes theorem. It outputs the Bayes rule, the confusion matrix, and the expected gain matrix associated with the input used. After completing the training and predicting steps, small perturbations are introduced in the measurement conditional probabilities of every misclassification, obtaining an improvement, as expected, in the Expected Gain (after recalculating all the classifier's outputs). The experiments included using the dataset split into equally sized training and test sets and using K-fold cross-validation. The experiments use synthetic probabilities distribution and create synthetic datasets. The appendix details the code implemented.

\end{abstract}

\section{Introduction}


The discrete Bayesian classifier is a popular predictive classifier broadly used in Machine Learning, based on the Bayes Theorem [Equation \ref{eq:bayes-theorem}].

\begin{equation}\label{eq:bayes-theorem}
  P(A \mid B) = \frac{P(B \mid A)\mathbin{}P(A)}{P(B)}
\end{equation}

A discrete Bayesian classifier, given a set of classifications, a measurement space, and a dataset, uses the Bayes theorem to compute the conditional probabilities and the Bayes rule decision needed to perform classification. It can be optimized using an Economic Gain Matrix by maximizing the Expected Gain obtained using the Bayes decision rule.

In our case, given a set $C$ of $K$ discrete classifications [Equation \ref{eq:classes-set}] and a discrete measurement space $D$, result of the cartesian product of $N$ discrete measurements $L_n$ [Equation \ref{eq:measurement-space}], the classifier is a function assigning a unique classification $c \in C$ to any measurement $\vec{d} \in D$.

\begin{equation}\label{eq:classes-set}
C = \{\,c_1,\, \dots,\, c_{K}\,\}
\end{equation}


\begin{gather}\label{eq:measurement-space}
D = \bigtimes_{n=1}^{N} L_n \\
L_n = \{l_{n_1},\, \dots,\, l_{n_{M_n}}\},\, \forall n \in { 1, ..., N }, M_n = \vert L_n\vert
\end{gather}

For the classification, the following inputs are required:

\begin{itemize}
  \item the discrete classification cardinality, $K$
  \item the cardinalities of each discrete measurement, $\vert L_n \vert,\, n \in N$
  \item an Economic Gain Matrix, $e^{K \times K}$
  \item a dataset with $Z$ measurements with matching $Z$ classifications. 
\end{itemize}

The Economic Gain matrix, $e$, is a $K \times K$ matrix that defines the gain (or cost) of making the right (or wrong) classifications [Equation \ref{eq:economic-gain-matrix}]. Each column will represent an assigned class, and each row will represent the true class. Then, every element $e(c_i,\, c_j) \in e$ represent the gain or cost of assigning the class $c_j$ when the true class was $c_i$. Usually, The Economic Gain is positive for every correct classification and non-positive for every other case. The identity matrix is an example of an economic gain matrix.

\begin{equation}\label{eq:economic-gain-matrix}
  e^{K \times K} = \begin{pmatrix}
    e(c_1,c_1) & \cdots & e(c_1,c_K) \\
      \vdots   & \ddots &    \vdots   \\
    e(c_K,c_1) & \cdots & e(c_K,c_K)
  \end{pmatrix}
\end{equation}


The economical consequences of the classifier are determined by the Expected Gain Matrix $G^{K \times K}$. Each element $g$ is the multiplication between the economic gain $e(i,\, j) \in K$ and the probability of the true class $c_i$ being assigned class $c_j$, $P(c_i,\, c_j)$ [Equation \ref{eq:expected-gain-matrix}].

\begin{equation}\label{eq:expected-gain-matrix}
  G^{K \times K} = \begin{pmatrix}
    e(c_1,c_1)P(c_1,c_1) & \cdots & e(c_1,c_K)P(c_1,c_K) \\
                \vdots   & \ddots &    \vdots   \\
    e(c_K,c_1)P(c_K,c_1) & \cdots & e(c_K,c_K)P(c_K,c_K)
  \end{pmatrix}
\end{equation}

The classifier's Expected Gain is the sum of all the Expected Gain Matrix's elements [Equation \ref{eq:expected-gain}].

\begin{equation}\label{eq:expected-gain}
  E[e] = \sum_{i \in K} \sum_{j \in K}e(c_i,\,c_k)\mathbin{}P(c_i,\,c_k)
\end{equation}

The goal is to determine the Bayes decision rule that maximizes the Expected Gain classifier, $E$.

A critical step in the construction of the classifier is the building of the Bayes decision rule $f_{\vec{d}}$ [Equation \ref{eq:bayes-rule}]. The Bayes decision rule will assign 1 to the class that maximizes the Expected Gain, and 0 otherwise [Equation \ref{eq:bayes-argmax}].

\begin{equation}\label{eq:bayes-rule}
  f_d(c_j) =
  \begin{cases}
  1 & j = k \\
  0 & j \neq k
  \end{cases}
\end{equation}

\begin{equation}\label{eq:bayes-argmax}
  \argmax_{c_k \in C} \sum_{j = 1}^{K} e(c_j, c_k)\mathbin{}P(c_j, \vec{d})
\end{equation}

In addition to the Economic Gain Matrix, a Bayes decision rule can refine the Economic Gain definition [\label{eq:bayes-expected-gain}].

\begin{equation}\label{eq:bayes-expected-gain}
  E[e, f] = \sum_{i \in K} \sum_{j \in K}\sum_{\vec{d} \in D}f_{\vec{d}}(c_j)\mathbin{}e(c_i,\, c_j)\mathbin{}P(c_i,\,\vec{d})
\end{equation}

To obtain $f_{\vec{d}}$ and $P(c,\, \vec{d})$, it is necessary to calculate the posterior probability of assigning a class $c$ to measurement $\vec{d}$, $P(c \mid \vec{d})$, using the prior class probability $P(c)$ and the probability of the measurement given the class $P(\vec{d} \mid c)$ [Equation \ref{eq:bayes-theorem-proportional}].

\begin{equation} \label{eq:bayes-theorem-proportional}
  P(c,\, \vec{d}) \mathbin{\propto} P(\vec{d} \mid c) \mathbin{} P(c)
\end{equation}

This report details an implementation of a discrete Bayesian classifier used to implement the classifier. A technical overview explains the design and implementation, along with the results of the experiments executed.

The definition and implementation include:

\begin{itemize}
  \item Classification and measurement dimensions, including pseudo-random probabilities and cumulative distribution functions
  \item Space definition and linear addresses
  \item Class prior and conditional probabilities
  \item Classifier optimization using an economic gain matrix
  \item Dataset definition and generation of pseudo-random synthetic data
  \item Experimentation framework with two types of cross-validation
\end{itemize}

The classifier's performance is tested by adding small perturbations to the class conditional, increasing the Expected Gain. Test-validation sets and V-folds Cross-Validation determine the increment.

\section{Technical}

\subsection{Definitions}

In order to explain the implementation, a few concepts need discussion.

\subsubsection{Dimension}
A dimension is a 1-dimensional set of $M$ correlatives integer numbers from 1 to $K$. Any 1-dimensional $L$ set has a Probability Mass Function $P$ and a Cumulative Distribution Function $Q$ [Equation \ref{fig:pmf-cdf}]. The $P$ set will be created by generating a set $R$ with $K$ pseudo-random numbers $r_1, ...r_K$ scaled to 1 $p_1, ...r_K$ [Equation \ref{eq:pmf}]. The $Q$ set is sequence of cumulative sum of the probabilities in the $P$ [Equation \ref{eq:cdf}].

\pgfplotstableread[row sep=\\,col sep=&]{
value & Probability \\
1 & 0.07155134805450836 \\
2 & 0.12997048848822798 \\
3 & 0.1386944981818227 \\
4 & 0.04197817941692496 \\
5 & 0.16869281585900325 \\
6 & 0.16136757309412517 \\
7 & 0.03288486428939286 \\
8 & 0.1571494317575896 \\
9 & 0.07917145577927193 \\
10 & 0.0185393450791332 \\
}\pmf

\pgfplotstableread[row sep=\\,col sep=&]{
value & Cumulative Probability \\
1 & 0.07155134805450836 \\
2 & 0.20152183654273634 \\
3 & 0.34021633472455903 \\
4 & 0.382194514141484 \\
5 & 0.5508873300004873 \\
6 & 0.7122549030946125 \\
7 & 0.7451397673840053 \\
8 & 0.9022891991415949 \\
9 & 0.9814606549208669 \\
10 & 1.0 \\
}\cdf

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      x label style={at={(axis description cs:0.5,-0.1)}, anchor=north},
      xlabel={Dimension values},
      legend pos=north west
      ]
      \addplot table[x=value,y=Probability]{\pmf};
      \addplot table[x=value,y=Cumulative Probability]{\cdf};
      \addlegendentry{PMF}
      \addlegendentry{CDF}
    \end{axis}
  \end{tikzpicture}
  \caption{PMF and CDF values for a dimension with 10 possible values}
\end{figure}

\begin{equation}\label{eq:pmf}
  \begin{aligned}
  R &= \{r_1, \dots, r_K\} \\
  P &= \Bigl\{p_i \mid p_i = \frac{r_i}{r_{\sum_{i = 1}^{K} r_i}}, r_i \in R \Bigr\}
  \end{aligned}
\end{equation}

\begin{equation}\label{eq:cdf}
Q = \Bigl\{q_i \mid q_i = \sum_{j = 1}^{K} p_j, p_j \in P \Bigr\}
\end{equation}

The classifications set $C$, and every measurement $L_n$ is a 1-dimensional set with their corresponding probabilities sets.

The cumulative distribution functions will be used to generate pseudo-random numbers in the 1-dimensional space: Given a cumulative distribution function $Q$ belonging to the dimension $M$, $f_Q$ will take a number $r \in [0, 1]$. $f_Q$ and assign it the number $k \in M$ according to the relative position of $r$ compared with the numbers in $Q$ [Equation \ref{eq:cumulative}].

\begin{equation}\label{eq:cumulative}
  f_Q(r) =
  \begin{cases}
  1 & r < q_1 \\
  k & q_{k - 1} < r \leq q_k,\, k \in M - \{1\} \\
  \end{cases}
\end{equation}

\subsubsection{Measurement Space}
The measurement space $D$ is the cartesian product of a set of measurements \label{eq:measurement-space}. Every element in $\vec{d} \in D$ is a vector where every value corresponds to a measurement value [Equation \ref{eq:measurements-vector}].

\begin{equation}\label{eq:measurements-vector}
  \vec{d} = (d_0, ..., d_N), \vec{d}_i \in L_n
\end{equation}

\subsubsection{Linear Space}
Each element in the measurement space can be mapped to a linear space using a bijective function $f_D$ that takes a vector and transform it to an integer value $l \in \mathcal{L}$ [Equation \ref{eq:linear-address}].

\begin{equation}\label{eq:linear-address}
  \begin{aligned}
  \mathcal{L} &=& \{1, \dots, \prod_{i \in N} \mid L_i \mid\} \\
  f_D&:& \bigtimes_{n=1}^{N} L_n \longleftrightarrow \mathcal{L} \\
  \end{aligned}
\end{equation}

We will assume every dimension in the measurement space is independent between each other, allowing us to compute the probability of every element in the measurement space $P(\vec{d})$ as the multiplication of the probabilities of each measurement value in $\vec{d}$ [Equation \ref{eq:prod:measurements}].

\begin{equation}\label{eq:prod:measurements}
  P(\vec{d}) = \prod_{i = 1}^{N} P(d_o), d_i \in L_n
\end{equation}

\subsubsection{Classifier}
The classifier is the element that assigns a classification to a measurement. The classifier emerges from the context of a set of classifications and a measurement space.

\subsubsection{Dataset}
A sample dataset is formed by two sets of equal size $Z$: the data $ X = \{x_i \in D \mid i \in [1, Z]\}$ and the target $Y = \{ y_i \in C \mid i \in [1, Z]\}$.

\subsubsection{Experiment}
An experiment is an isolated routine that takes inputs and produces results. The experiments will define dimensions, classifications, measurement space, classifier, and datasets.

An experiment uses immutable measurement space and classifications until the end of the whole process. That condition will mean that, during an experiment, the cumulative probabilities distribution remains unchanged. The experiments will yield a classifier that will work over the measurement space and classifications in the experiment.

\subsubsection{Iteration}
An iteration within an experiment will enclose the use and modification of the classifier available. Each iteration in a sequence of iterations will include the classifier's changes by the previous iteration.

\subsubsection{Validation}
An iteration has two kinds of testing and validation processes:
\begin{itemize}
  \item Test-Validation sets: The sample dataset will be shuffled and divided into equal-sized subsets. An iteration uses the test subset to predict and adapt; the iteration finishes using the validation set [Equation \ref{eq:test-validation}].
  \item V-fold sets: The sample dataset will be shuffled and divided into $V$ equal-sized folds. A V round-robin iterations will select a different fold as a validation set, and it will join the resting folds as a testing set [Equation \ref{eq:v-fold}].
\end{itemize}

\begin{figure}
  \begin{tikzpicture}
    \matrix (M) [matrix of nodes,
        nodes={minimum height = 5mm, minimum width = 2.3cm, outer sep=0, anchor=center, draw},
        column 1/.style={nodes={draw=none}, minimum width = 2.3cm},
        row sep=1mm, column sep=-\pgflinewidth, nodes in empty cells,
        e/.style={fill=yellow!10}
      ]
      {
        Iteration & |[e]| & \\
      };
    \draw (M-1-2.north west) ++(0,2mm) coordinate (LT) edge[|<->|, >= latex] node[above]{Sample Dataset} (LT-|M-1-3.north east);
  \end{tikzpicture}
  \caption{Test-Validation cross-validation.}
  \label{eq:test-validation}
\end{figure}

\begin{figure}\label{eq:v-fold}
  \begin{tikzpicture}
    \matrix (M) [matrix of nodes,
        nodes={minimum height = 5mm, minimum width = 1cm, outer sep=0, anchor=center, draw},
        column 1/.style={nodes={draw=none}, minimum width = 1cm},
        row sep=1mm, column sep=-\pgflinewidth, nodes in empty cells,
        e/.style={fill=yellow!10}
      ]
      {
        Iteration 1 & |[e]| & & & & \\
        Iteration 2 & & |[e]| & & & \\
        Iteration 3 & & & |[e]| & & \\
        Iteration 4 & & & & |[e]| & \\
        Iteration 5 & & & & & |[e]| \\
      };
    \draw (M-1-2.north west) ++(0,2mm) coordinate (LT) edge[|<->|, >= latex] node[above]{Sample Dataset} (LT-|M-1-6.north east);
  \end{tikzpicture}
  \caption{5-fold cross-validation. Each iteration uses the yellow fold as validation set and the rest folds as test set.}
\end{figure}
  

\subsection{Discrete Bayesian Classifier}

A classifier will assign a classification $c \in C$ to any measurement $\vec{d} \in D$ from the measurement space. Within the classifier, all measurement will be translated to the corresponding linear address. The following parameters will be set:

\begin{itemize}
  \item The list of $S$ linear addresses $l(\vec{d})$ for each $\vec{d} \in D$ [Equation \ref{eq:linear-address}], stored in a $S$ sized vector
  \item Measurement probability $P(\vec{d})$ for each  $\vec{d} \in D$ [Equation \ref{eq:prod:measurements}], stored in a $S$ sized vector
  \item Class probability $P(c)$ for each  $c \in C$ [Equation \ref{eq:pmf}], stored in a $K$ sized vector
  \item Class conditional probabilities $P(\vec{d} \mid c)$. Each class $c \in C$ is assigned a probability mass distribution with $S$ probabilities [Equation \ref{eq:pmf}], stored in a $K \times S$ matrix
\end{itemize}

After initializing the classifier, given an Economic Gain Matrix [Equation \ref{eq:economic-gain-matrix}], the classifier can compute the elements needed to classify a measurement:

\begin{itemize}
  \item $P(c \mid \vec{d})$, the probabilities of class given a measurement, stored in an $S \times K$ matrix [Equation \ref{eq:bayes-theorem}]
  \item $P(c,\, \vec{d})$ the probabilities of a class and a measurement, stored in an $S \times K$ matrix [Equation \ref{eq:bayes-theorem-proportional}]
  \item the Bayes decision rule that optimize the expected gain for the Economic Gain Matrix, stored in an $S \times K$ matrix [Equation \ref{eq:bayes-rule}]
  \item The Confusion Matrix, stored in an $K \times K$ matrix
  \item The Expected Gain Matrix, stored in an $K \times K$ matrix [Equation \ref{eq:expected-gain-matrix}]
  \item The Expected Gain, the trace of the Expected Gain Matrix
\end{itemize}

The process will end with a discrete Bayesian classifier optimized to maximize Expected Gain given an Economic Gain Matrix.

\section{Experiment}\label{experiment}

To validate the discrete Bayesian classifier, a series of iteration will modify the probabilities of measurement given a class $P(\vec{d} \mid c$ by adding small perturbations. Each perturbation must increase the Expected Gain monotonically to 1.

Each iteration in the experiment will compute a classifier optimized for a specific Economic Gain Matrix. Using that classifier dataset of $Z$ samples measurements and corresponding classifications, generating random measurements $\vec{d}$ and assign a random $c$ using as probability mass distribution $P(c \mid \vec{d})$. $Z$ will be define 10 times the classifications number times the measurement space [Equation \ref{eq:sample-size}].

\begin{equation}\label{eq:sample-size}
  Z = 10 \times \mid C \mid \times \prod_{n \in N} |L_n|
\end{equation}

The classifier assigns a classification to each measurement in the test sample set. Based on those results, the conditional probabilities given a class will be modified, aiming that in the next iteration, the data generated should conform more alike to the classifications. The adaptation follows these steps:

\begin{itemize}
  \item For each assigned classification $c'$ made, if the classification is wrong a $\Delta$ perturbation will be added to $P(\vec{d} \mid c')$\footnote{This is different to what was instructed in the midterms slides \cite{midterm-project}. Section \ref{problems} provides further information.}.
  \item Normalize to 1 each column on the $P(\vec{d} \mid c)$ matrix
  \item Compute the classifier with the new probabilities
  \item Return the Expected Gain
\end{itemize}

The next iteration must do the same steps using the updated classifier and yield a higher Expected Gain.

\section{Experiment Results}

A base experiment will have the following default setup:

\begin{itemize}
  \item $K = 2$ classifications
  \item $N = 2$ measurements
  \item $M_n = 2, n \in N$ values for each measurement
  \item $e^{K \times K} = I_K$ as Economic Gain Matrix
  \item $\Delta = 0.01$ as a probability perturbation
  \item $R = 10$ iterations per experiment
  \item $V = 2$ folds (test/validation cross-validation)
\end{itemize}

\subsection{Default Setup}

The default case generates $Z = 80$ samples. In $R = 10$ iterations the Expected Gain goes from $0.6217987808286073$ to $0.7389581277559807$.

\subsection{Testing the Parameters}

Increasing the classifications dimension's size while keeping the rest of the default values generates a decrement in the Expected Gain [Figure \ref{fig:results-K-N2-M2-R10-D001}].

\input{figures/default-K-N2-M2-R10-D001}

Increasing the amount of measurements dimensions generates improvements on the Expected Gain but also a notorious increase of the samples generated [Figure \ref{fig:results-K2-N-M2-R10-D001}]

\input{figures/default-K2-N-M2-R10-D001}

Increasing the number of values per measurement generates improvements on the Expected Gain but also a notorious increase in the samples generated [Figure \ref{fig:results-K2-N2-M-R10-D001}]

\input{figures/default-K2-N2-M-R10-D001}

In $R = 10$ iterations the Expected Gain goes from $0.6217987808286073$ to $0.7389581277559807$. For $R > 400$ the Expected Gain gets stuck in $0.8088281174549596$ [Figure \ref{fig:results-K2-N2-M2-R-D001}]

\input{figures/default-K2-N2-M2-R-D001}

The Expected Gain's increases faster using the default perturbation value $10^{-2}$ than using smaller ones [Figure \ref{fig:results-K2-N2-M2-R10-D}].

\input{figures/default-K2-N2-M2-R10-D}

All examples discussed above use the same probabilities mass distributions when using the same values of $K$, $M$ or $L$. For example, in every case where $K = 2$ the distribution is $P(c) = \begin{pmatrix}0.6137987267145945\\0.3862012732854055\end{pmatrix}$.

\subsection{More classifications, measurements and iterations}

The measurement space size grows exponentially with the number of values in each measurement [Figure \ref{fig:results-K2-N-M2-R10-D001}]. Using $k =10$ classifications, $N = 5$ measurements each one with $M = 5$ values, the sample size has a size of $Z = 312,500$ samples. Running $R = 100$ iterations takes about $1300$ seconds, about $4000$ times the default case time for the same number of iterations. Again, the performance is better with perturbation sized $\Delta = 0.01$ [Figure \ref{fig:custom-K10-N5-M5-R100-D}].

\input{figures/custom-K10-N5-M5-R100-D}

In a hundred iterations, the best Expected Gain achieved is $0.9995150612331569$, using a $\Delta = 0.01$. In a thousand iterations, the Expected Gain raises to $0.9999999999806929$ [Figure \ref{fig:custom-K10-N5-M5-R1000-D001}].

\input{figures/custom-K10-N5-M5-R1000-D001}


\subsubsection{V-fold cross-validation}

All the previous experiments divided the sample set into a test set and a validation set. V-fold validation allows using more data for the testing, so it is expectable to produce better results in fewer iterations [Figure \ref{fig:custom-K6-N4-M3-R1000-D001-V}].

\input{figures/custom-K6-N4-M3-R1000-D001-V}

\section{Problems}\label{problems}

In Section \ref{experiment}, at the experiment definition, it is described that the $\Delta$ perturbation modifies to the probability of the measurement $\vec{d}$ given the assigned class $c'$ that differs from the actual class $c$ from the dataset used. The perturbation increases the probability generation of the combination $(c', \vec{d})$ in the dataset of the next iteration, at the expense of any other class $c \neq c'$, making that dataset more alike to the Bayes decision rule. That differs from the instructions provided in \cite{midterm-project} slide 24, where it is detailed that the perturbation changes the correct class $c$ conditional probability (and not the assigned class $c'$).

Using the original instructions, the Expected Gain does not show a monotonic increase as it is supposed to do. Instead, it gets stuck bouncing between different values [Figure \ref{fig:slides-K5-N4-M3-R100-D001-V3}]. An explanation for this could be this happens because by doing that change, in every iteration, we force the generation of more samples assigned incorrectly in the previous step, forcing the Bayes rule decision to change its decision in every iteration.

\input{figures/slides-K5-N4-M3-R100-D001-V3}


\section{Conclusions}


This report showed the definition, implementation, and experimentation of a discrete Bayes classifier.

A discrete Bayes classifier involves measurement space formed by multiple discrete measurements. It assigns each measurement a classification based on what the Bayes decision rule determines as the classification that maximizes the classifier's Expectation Gain.

The critical step of the classifier is the construction of the Bayes decision rule. In this report, the probabilities $P(c)$, $P(d \mid c)$, and an Economic Gain Matrix act as the process's input. The classifier reaches its optimum by choosing a Bayes decision rule that maximizes the Expected Gain, based on the Economic Gain Matrix used as input.

The classifier's validation involves generating synthetic datasets based on the conditional probabilities computed by the classifier. During the experimentation phase, and using the Identity Matrix as the Economic Gain Matrix, the classifier's Expected Gain tends to 1.0 when, in repeated iterations, the probabilities are modified, adding small perturbation to the class conditional probabilities. The modification will move the synthetic datasets closer to the Bayes rule decision each time the classifier generates new data.

Multiple experiments run using test-validation sets and V-fold cross-validation, achieving better results in less iteration using the cross-validation approach.

The implementation described in this report does not follow precisely the instructions from the midterm assignment because the interpretation of that instruction did not monotonically increase the Expected Gain. Instead, an alternative is proposed, achieving the monotonic increase desired. The changes involve adding the perturbations to the class conditional probabilities of the assigned class instead of the actual class, as suggested in the assignment.

The code developed and used is available at \url{https://github.com/afaundez/ml-midterm}. The appendix describes the concepts in the code.


\appendix


To run the code, a Unix environment with Ruby 2.6+ is necessary. The code is available on GitHub at \url{https://github.com/afaundez/ml-midterm}.

To test the command and check the available options run:

\begin{verbatim}
  $ ./midterm --help
\end{verbatim}

\subsection{Options}

For testing, the following parameters are available for configuration:

\begin{lstlisting}[language=sh]
  Usage: midterm [options]
    -s [INT] Pseudo-random seed
    -K [INT] Class cardinality
    -N [INT] Measurements cardinality
    -M [INT] Measurement cardinality 
    -Z [INT] Sample size
    -R [INT] Iterations
    -D [FLOAT] Perturbation size
    -V [FLOAT] Number of folds
\end{lstlisting}

Example: the command

\begin{lstlisting}[language=sh]
  bin/midterm -s1 -K2 -N2 -M2 -R10 -D0.01
\end{lstlisting}

Runs ten iterations using two classifications, two measurements (each one with two values), and add perturbations of 0.01.

\subsection{Runtime}

\subsubsection{Build}

The build uses three significant abstractions: Dimension, Space, and DataSet.

The Dimension abstraction stores the cardinality, probabilities distribution function, and cumulative distribution function of a single measurement or class. A dimension does not store values, but it can generate random values using the distribution functions. A Dimension is defined by:

\begin{itemize}
  \item size
  \item pmf, the probability mass function
  \item cdf, the cumulative distribution function
\end{itemize}

The Space abstraction stores a collection of measurements dimensions \ref{dimensions}. It is in charge of transforming a measurement vector into a linear address and vice-versa. Space is defined by:

\begin{itemize}
  \item dimensions, a collection of Dimension instances
\end{itemize}

The DataSet abstraction generates and stores measurement values and the associated classification; all bounded to a specific space and classification labels. A DataSet is defined by: 

\begin{itemize}
  \item data, a collection measurement space values
  \item target, a collection of classification labels
\end{itemize}

The classifier is in charge of the training and improving the class conditional probabilities, determining the Bayes Decision Rule, generating the Confusion Matrix, and calculating the Expected Gain.

\begin{itemize}
  \item space
  \item labels (classification dimensions)
  \item expected gain matrix (default to the identity matrix)
\end{itemize}

Using the Ruby language characteristics, the solution provides dynamic specific language specifically created for this problem. As with almost every code in Ruby, it came with a performance trade-off, but it attempts to make the code more human-readable.

\begin{thebibliography}{2}

\bibitem{discrete-bayes}
Robert M. Haralick,
\textit{Discrete Bayes Pattern Recognition}
\\\texttt{\url{http://haralick.org/ML/discrete_bayes.pdf}}

\bibitem{midterm-project}
Robert M. Haralick,
\textit{Midterm Project}
\\\texttt{\url{http://haralick.org/ML/midterm_project.pdf}}

\end{thebibliography}

\end{document}
